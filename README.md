# YVES â€“ Playfully Teaching AI Principles through Interactive Machine Teaching

## ğŸ“– Overview

This project started in June 2022 and was completed in February 2023

## ğŸ“– Overview

**YVES** is a playful, tangible AI teaching prototype developed during my Bachelor's thesis at Ludwig-Maximilians-UniversitÃ¤t (LMU) Munich under the Human-Centered Ubiquitous Media group.  
It teaches non-experts â€” especially young users â€” fundamental principles of Artificial Intelligence (AI) and Machine Learning (ML) through hands-on interaction with an object detection system.

YVES was first evaluated at LMU's Open Lab Day and later exhibited at the Copernicus Science Centre (Warsaw, Poland), demonstrating that playful interaction can significantly improve AI understanding among the public.  
In 2023, the Ubiquitous Media group further developed the system and deployed it at the Deutsches Museum (Munich).

## About

Through a museum-style experience, YVES lets users:
- Create their own training datasets.
- Simulate AI model training.
- Test object detection performance.
- Understand why diversity and quality in data are critical for AI success.

Built with **Flask**, **TensorFlow**, and **YOLO**, YVES uses pre-trained object detection models to demonstrate how AI systems learn â€” and why they sometimes fail.

## ğŸ¯ Motivation

Despite AI's growing influence in daily life, public understanding of its workings remains limited.  
YVES addresses this gap by offering an interactive, gamified machine teaching experience that transforms users into active AI "teachers" â€” building better awareness of training data, learning processes, and AI system limitations.


## ğŸ§ª Features

- Hands-on half simulated object detection model training.
Two interaction types:
- Guided Interaction (automated dataset augmentation).
- Unguided Interaction (user-driven dataset diversification).
- Real-time feedback through bounding boxes and detection accuracy scores.
- Designed for museum-style exhibits and public short term engagement with young viewers.

## ğŸ› ï¸ Technologies Used

- Flask â€“ backend web framework.
- TensorFlow â€“ object detection model training.
- OpenCV â€“ camera capture and processing.
- HTML/CSS/JavaScript â€“ frontend interface.
- YOLO (You Only Look Once) â€“ real-time object detection.


## ğŸ“¦ Project Structure

```
â”œâ”€â”€ app.py            # Main Flask application
â”œâ”€â”€ models/           # Pre-trained object detection models
â”œâ”€â”€ templates/        # HTML templates
â”œâ”€â”€ static/           # CSS, JavaScript, and static files
â”œâ”€â”€ uploads/          # Temporary storage for user-uploaded images
â”œâ”€â”€ utils/            # Helper functions (if applicable)
â””â”€â”€ requirements.txt  # Python dependencies
```

## Future Work

- Real-time training (if computational constraints are overcome).
- More AI principles (e.g., bounding box accuracy, adversarial examples).

## Author

Anna Papanakli
